#+TITLE: Feature Extraction
#+SUBTITLE: GRSS Summer School
#+AUTHOR: Mathieu Fauvel
#+EMAIL: mathieu.fauvel@ensat.fr
#+DATE: [2017-04-26 Wed 10:30-12:00]

#+INCLUDE_TAGS: export
#+EXCLUDE_TAGS: noexport
#+LANGUAGE: en
#+OPTIONS: H:3 toc:t tags:nil properties:nil

#+COLUMNS: %40ITEM(Task) %17Effort(Estimated Effort){:} %CLOCKSUM

#+LaTeX_CLASS_OPTIONS: [10pt,aspectratio=1610]

#+BEAMER_THEME: DarkConsole
#+BEAMER_HEADER: \institute{UMR Dynafor}
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsection]\end{frame}}
#+BEAMER_HEADER: \AtBeginSubsection[]{\begin{frame}<beamer>\frametitle{Outline}\tableofcontents[currentsubsection]\end{frame}}
#+BEAMER_HEADER: \setbeamercovered{again covered={\opaqueness<1->{25}}}
#+BEAMER_HEADER: \usefonttheme[onlymath]{serif}

#+LATEX_HEADER: \usepackage[english]{babel}\usepackage{etex}\usepackage{minted}\usemintedstyle{emacs}
#+LATEX_HEADER: \usepackage{tikz}\usepackage{amsmath}\usepackage[T1]{fontenc}\usepackage{lmodern}%\usepackage{arev}
#+LATEX_HEADER: \usepackage{booktabs}\usepackage[citestyle=alphabetic,bibstyle=authortitle]{biblatex}
#+LATEX_HEADER: \usepackage{pgfplots,pgfplotstable}\usetikzlibrary{pgfplots.groupplots}\usepackage[babel=true,kerning=true]{microtype}\usepackage{smartdiagram}
#+LATEX_HEADER: \addbibresource{fe.bib}
#+LATEX_HEADER: \usetikzlibrary{mindmap,trees,shapes,arrows,spy,3d,decorations.pathmorphing,pgfplots.statistics,pgfplots.dateplot}
#+LATEX_HEADER: \pgfplotsset{compat=newest}
* Motivations                                                        :export:
*** Illustration
- <1-> *Curse of dimensionality*: it is not possible to get enough data to cover all the observation space.
  #+BEGIN_CENTER
  /High dimensional saces are mostly empty !/
  #+END_CENTER
- <2> Multivariate data live in a lower dimensional space
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[grid=major,small]
        \addplot3 [mesh, samples=15, domain=-5:5] {x+y+1};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** Application
- Feature extraction is important in remote sensing because:
  + It reduces the size of the data,
  + It limits the spatial and spectral redundancy,
  + It permits visualization of the data,
  + It mitigates the /curse of dimensionality/.
- Extraction techniques:
  + Spectral
    * Physically based method,
    * Statistical methods.
  + Spatial:
    * Linear filters,
    * Non linear techniques (Mathematical Morphology)

    
* Physical Indices                                                   :export:
** Introduction
*** Spectral indices
- Spectral indices are a linear/non-linear combination of two (or more) spectral bands.
- They provides information as a /single number/ about:
  + Plant structure,
  + Biochemistry,
  + Humidity,
  + Stress.
- Four main types cite:hrsv:2011:
  #+ATTR_LATEX: :centering :booktabs t
  | Name                                   | Formulae                                                                            |
  |----------------------------------------+-------------------------------------------------------------------------------------|
  | Difference vegetation index            | $R_{\lambda_1} - R_{\lambda_2}$                                                   |
  | Ratio vegetation index                 | $\dfrac{R_{\lambda_1}}{R_{\lambda_2}}$                                            |
  | Normalized difference vegetation index | $\dfrac{R_{\lambda_1} - R_{\lambda_2}}{R_{\lambda_1} + R_{\lambda_2}}$          |
  | Soil-adjusted vegetation index         | $(1+L)\times\dfrac{R_{\lambda_1} - R_{\lambda_2}}{R_{\lambda_1} - R_{\lambda_2}+L}$ |
- /The three last indexes are invariant to  a multiplicative factor/

*** Conventional Indices
Index database : [[http://www.indexdatabase.de/]]

#+ATTR_LATEX: :centering :booktabs t :font \small
| Name                                        | Formulae  ($\lambda$ nm)                                                                                                                        |
|---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| Normalized Difference Vnegetation index     | $\dfrac{R_{\lambda_{800}} - R_{\lambda_{670}}}{R_{\lambda_{800}} + R_{\lambda_{670}}}$                                                          |
| Modified Soil-Adjusted Vegetation Index     | $\dfrac{1}{2}\left[2R_{\lambda_{800}}+1 - \sqrt{(2R_{\lambda_{800}}+1)^2-8(R_{\lambda_{800}}-R_{\lambda_{670}})}\right]$                        |
| Modified Chlorophyll Absorption Ratio Index | $\left[(R_{\lambda_{700}}-R_{\lambda_{670}})-0.2(R_{\lambda_{700}}-R_{\lambda_{550}})\right]\times\dfrac{R_{\lambda_{700}}}{R_{\lambda_{670}}}$ |
|---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| Normalized Difference Water Index           | $\dfrac{R_{\lambda_{858}} - R_{\lambda_{1240}}}{R_{\lambda_{858}} + R_{\lambda_{1240}}}$                                                        |
| Datt Reflectance Index                      | $\dfrac{R_{\lambda_{816}} - R_{\lambda_{2218}}}{R_{\lambda_{816}} + R_{\lambda_{2218}}}$                                                        |
|---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------|
| Normalized Difference Redness Index         | $\dfrac{R_{\lambda_{540}} - R_{\lambda_{700}}}{R_{\lambda_{540}} + R_{\lambda_{700}}}$                                                          |
| Soil Brightness Index                       | $0.406R_{\lambda{550}}+0.600R_{\lambda{650}}+0.645R_{\lambda{750}}+0.243R_{\lambda{950}}$                                                       |

** Vegetation Indices
*** Normalized difference vegetation index
#+BEGIN_EXPORT latex
$$\text{NDVI}=\dfrac{R_{\lambda_{800}} - R_{\lambda_{670}}}{R_{\lambda_{800}} + R_{\lambda_{670}}}$$
#+END_EXPORT
- $-1\leq \text{NVDI} \leq 1$
- $\text{NDVI}< 0$: surfaces other thatn plant cover
- $\text{NDVI}\approx 0$: bare soil
- $\text{NDVI}\geq 0.1$: vegetation cover (higher values correspond to more dense covers)

#+BEGIN_EXPORT latex
\begin{center}
\begin{tikzpicture}
\begin{axis}[xmin=0.4,xmax=1,ymin=0,ymax=1,grid,xlabel=$\lambda~({\mu}m)$,ylabel=Reflectance,width=0.6\linewidth,height=0.3\linewidth,cycle list name=color list]
  \addplot+[mark=none,thick,smooth] file {../Introduction/figures/oak.txt};
  \pgfplotstableread{../Introduction/figures/grass.txt}\loadedtable
  \addplot+[mark=none,smooth,thick] table[x=wave,y=grass] from \loadedtable;
  \addplot+[mark=none,smooth,thick] table[x=wave,y=drygrass] from \loadedtable;
  \pgfplotstableread{../Introduction/figures/talc.txt}\loadtable
  \addplot+[mark=none,smooth,thick] table[x=wave,y=talc] from \loadtable;
  \legend{0.81,0.90, 0.05, -0.03}
\end{axis}
\end{tikzpicture}
\end{center}
#+END_EXPORT
** Case study
*** University of Pavia
**** Images                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[file:./figures/university_color.png]]

**** Parameters                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
- Peri-urban area
- Rosis-3 sensor
- 103 Spectral bands (400nm-900nm)
- 1.5 meter per pixel spatial resolution
- 610 $\times$ 340 pixels

*** Orfeo-Toolbox
- [[https://www.orfeo-toolbox.org/][OTB]] is a C++ library for remote sensing images processing.
- It is free, open-source and available for most OS (window, apple, linux)
- [[https://www.orfeo-toolbox.org/CookBook/OTB-Applications.html][OTB-Applications]] are set of tools appropriated for big/large images
- They are avalaible from QGIS, Python and Bash
- To compute the NDVI

#+BEGIN_SRC bash :tangle ../Codes/spectral_indices.sh
# Computation of the NDVI
otbcli_BandMath -il ../Data/university.tif -out ../Data/university_ndvi.tif \
		-exp "(im1b83-im1b56)/(im1b83+im1b56)"

# Computation of the SBI
otbcli_BandMath -il ../Data/university.tif -out ../Data/university_sbi.tif \
		-exp "0.406*im1b31 + 0.6*im1b52 + 0.645*im1b73"
#+END_SRC

*** University of Pavia - Spectral Indices
**** Images                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width \linewidth
[[file:./figures/university_color.png]]

**** NDVI                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width \linewidth
[[file:./figures/university_ndvi.png]]

**** SBI                                                           :BMCOL: 
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width \linewidth
[[file:./figures/university_sbi.png]]

*** Where is the vegetation 1/2 ?

  #+BEGIN_EXPORT latex
    \begin{center}
    \begin{tikzpicture}
      \begin{axis}[grid=both,width=0.95\linewidth,height=0.45\linewidth,/pgf/number format/1000 sep={},/pgf/number format/fixed,title=Density plot of the NDVI,xmin=-0.6,xmax=1,ymin=0,ymax=0.01]
        \addplot+[mark=none,thick,smooth] table[x=x,y=y,col sep=comma] {figures/pdf.csv};
        \only<2->{\addplot[red,thick] coordinates {(0.19,0) (0.19,0.008)};
        \addplot[red,thick] coordinates {(0.62,0) (0.62,0.008)}; }     
      \end{axis}
  \end{tikzpicture}
  \end{center}
  #+END_EXPORT

#+BEGIN_SRC bash :tangle ../Codes/spectral_indices.sh
# Segmentation of the NDVI in three classes
otbcli_BandMath -il ../Data/university_ndvi.tif -out ../Data/university_ndvi_segmented.tif \
		-exp "(im1b1<0.19?1:(im1b1<0.62?2:3))"
#+END_SRC

*** Where is the vegetation 2/2 ?
**** Images                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[file:./figures/university_color.png]]

**** NDVI                                                          :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+ATTR_LATEX: :width 0.6\linewidth
[[file:./figures/university_ndvi_segmented.png]]

** Question
*** Could you find the good one ?
#+BEGIN_EXPORT latex
 \centerline{\begin{tabular}{cc}
    \includegraphics[width=0.4\linewidth]{figures/image1.jpg} & \begin{tikzpicture}\pgfplotsset{every axis legend/.append style={at={(0.5,1.03)},anchor=south}}
      \begin{axis}[ytick=\empty,xmin=-0.5,xmax=0.9,ymin=0,width=0.5\linewidth,axis y line=center,axis x line=bottom,legend columns=4]
        \pgfplotstableread{figures/ndvi1.txt}\loadedtable
        \addplot[smooth,very thick,dashed,blue] table[x=wave,y=ndvi] from \loadedtable;
        \pgfplotstableread{figures/ndvi2.txt}\loadedtable
        \addplot[smooth,very thick,magenta] table[x=wave,y=ndvi] from \loadedtable;
        \pgfplotstableread{figures/ndvi3.txt}\loadedtable
        \addplot[smooth,very thick,dotted,orange] table[x=wave,y=ndvi] from \loadedtable;
        \pgfplotstableread{figures/ndvi4.txt}\loadedtable
        \addplot[smooth,very thick,dashdotted,green] table[x=wave,y=ndvi] from \loadedtable;
        \legend{ndvi$_1$,ndvi$_2$,ndvi$_3$,ndvi$_4$};
      \end{axis}
    \end{tikzpicture}\\
    Image & NDVI Histogram
\end{tabular}}
#+END_EXPORT
#+LaTeX: \vspace{1cm}

From the histogram, which one does correspond to the NDVI of the image ?
* Statistical Feature Extraction                                     :export:
** Unsupervised
*** Principal Components Analysis
- Linear transformation used to reduce the dimensionality of the data cite:jolliffe2002principal.
  $$ z_i = \langle\mathbf{v}_i,\mathbf{x}\rangle$$
- Find features $\mathbf{z}$ that  account for most of the variability of the data:
  + $z_1,~z_2,~z_3,\ldots$ are mutually uncorrelated,
  + $\text{var}(z_i)$ is as large as possible,
  + $\text{var}(z_1)>\text{var}(z_2)>\text{var}(z_3)>\ldots$

#+BEGIN_EXPORT latex
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[grid,small,width=0.4\linewidth,height=0.32\linewidth,xmin=0,xmax=2.5,ymin=0,ymax=2]
      \addplot[only marks,blue] table[x index=0,y index = 1,col sep =comma] {figures/pca_data.csv};
      \begin{scope}
      \addplot[very thick,red] coordinates { (0.080264,0.83834891)  (2.06023219,1.12070676)};
      \addplot[very thick,red] coordinates { (0.92906917,  1.96951193)(1.21142702, -0.01045626)};
    \end{scope}

   \end{axis}                                  
  \end{tikzpicture}
\end{center}
#+END_EXPORT
*** Maximization of the variance 1/2
- <1-> Search $\mathbf{v}_1$ such as $\max\text{var}(z_1)$:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \text{var}(z_1) & = & \text{var}(\langle\mathbf{v}_1,\mathbf{x}\rangle)\\
    &=& \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <2-> Indetermined: if $\hat{\mathbf{v}}_1$ maximizes the variance, $\alpha\hat{\mathbf{v}}_1$ too!  Add a constraint: $\langle\mathbf{v}_1,\mathbf{v}_1\rangle=1$
- <3-> Lagrangian:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathcal{L}(\mathbf{v}_1,\lambda_1) = \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_1 + \lambda_1(1- \mathbf{v}_1^\top\mathbf{v}_1)  
  \end{eqnarray*}
  #+END_EXPORT
- <4-> Compute the derivative w.r.t $\mathbf{v}_1$:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
  \frac{\partial\mathcal{L}}{\partial\mathbf{v}_1} = 2\boldsymbol{\Sigma}\mathbf{v}_1-2\lambda_1\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <5-> $\mathbf{v}_1$ is an eigenvector of the covariance matrix of $\mathbf{x}$:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \boldsymbol{\Sigma}\mathbf{v}_1 =\lambda_1\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <6->  $\mathbf{v}_1$ is the eigenvector corresponding to the largest eigenvalues !
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \text{var}(z_1)  =  \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_1 = \lambda_1 \mathbf{v}_1^\top\mathbf{v}_1 = \lambda_1
  \end{eqnarray*}
  #+END_EXPORT
*** Maximization of the variance 2/2
- <1-> Search $\mathbf{v}_2$ such as $\max\text{var}(z_2)$ and $\langle\mathbf{v}_2,\mathbf{v}_2\rangle=1$ and $\langle\mathbf{v}_1,\mathbf{v}_2\rangle=0$
- <2-> Lagrangian:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathcal{L}(\mathbf{v}_2,\lambda_2,\beta_1) = \mathbf{v}_2^\top\boldsymbol{\Sigma}\mathbf{v}_2 + \lambda_1(1- \mathbf{v}_2^\top\mathbf{v}_2) + \beta_1(0 - \mathbf{v}_2^\top\mathbf{v}_1)
  \end{eqnarray*}
  #+END_EXPORT
- <3-> Compute the derivative w.r.t $\mathbf{v}_2$:
   #+BEGIN_EXPORT latex
  \begin{eqnarray*}
  \frac{\partial\mathcal{L}}{\partial\mathbf{v}_2} &=& 2\boldsymbol{\Sigma}\mathbf{v}_2-2\lambda_1\mathbf{v}_2-\beta_1\mathbf{v}_1\\
  \boldsymbol{\Sigma}\mathbf{v}_2 &=& \lambda_1\mathbf{v}_2+2\beta_1\mathbf{v}_1
  \end{eqnarray*}
  #+END_EXPORT
- <4-> At optimality, $\langle\mathbf{v}_1,\mathbf{v}_2\rangle=0$. Left-multiplying by $\mathbf{v}_1^\top$ the above equation:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathbf{v}_1^\top\boldsymbol{\Sigma}\mathbf{v}_2 &=& 2\beta_1 \\
    \lambda_1\mathbf{v}_1^\top\mathbf{v}_2 &=& 2\beta_1 \\
    0 &=& 2\beta_1 \\
  \end{eqnarray*}
  #+END_EXPORT
- <5-> Hence, we have 
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \boldsymbol{\Sigma}\mathbf{v}_2 =\lambda_2\mathbf{v}_2
  \end{eqnarray*}
  #+END_EXPORT
- <6-> $\mathbf{v}_2$ is the eigenvector corresponding the /second largest/ eigenvalues
- <7-> $\mathbf{v}_k$ is the eigenvector corresponding the /$k^{\text{th}}$ largest/ eigenvalues
*** PCA in practice
1) Empirical estimation the mean value:
   #+BEGIN_EXPORT latex
   \begin{eqnarray*}
     \boldsymbol{\mu} = \frac{1}{n}\sum_{i=1}^n\mathbf{x}_i
   \end{eqnarray*}
   #+END_EXPORT
2) Empirical estimation the covariance matrix:
   #+BEGIN_EXPORT latex
   \begin{eqnarray*}
     \boldsymbol{\Sigma} = \frac{1}{n-1}\sum_{i=1}^n(\mathbf{x}_i-\boldsymbol{\mu})(\mathbf{x}_i-\boldsymbol{\mu})^\top
   \end{eqnarray*}
   #+END_EXPORT
3) Compute $p$ first eigenvalues/eigenvectors... How to choose $p$ ? Explained variance: 
   #+BEGIN_EXPORT latex
   $$\frac{\sum_{i=1}^p\lambda_i}{\sum_{i=1}^d\lambda_i}$$
   #+END_EXPORT
4) Tips for high dimensional data set: if $n<d$ see cite:manolakis2016hyperspectral page 420
*** PCA case study 1/3
#+BEGIN_SRC python :tangle ../Codes/pcaPavia.py
import rasterTools as rt
import scipy as sp
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load data set
im,GeoT,Proj = rt.open_data('../Data/university.tif')
[h,w,b]=im.shape
im.shape=(h*w,b)
wave = sp.loadtxt('../Data/waves.csv',delimiter=',')

# Do PCA
pca = PCA()
pca.fit(im)

# Plot explained variance
l = pca.explained_variance_ratio_
print l[:5]
print (l.cumsum()/l.sum())[:5]

# Save Eigenvectors
D = sp.concatenate((wave[:,sp.newaxis],pca.components_[:3,:].T),axis=1)
sp.savetxt('../FeatureExtraction/figures/pca_pcs.csv',D,delimiter=',')
#+END_SRC
*** PCA case study 2/3
- Explained variance
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,ylabel=\% of variance,xlabel=Number of principal components,axis y line*=left,yticklabel style=red,ylabel style=red, y axis line style=red,ytick style=red]
        \addplot[thick,mark=*,red] coordinates { (1,0.58318064)  (2,0.94418758)  (3,0.98856319)  (4,0.99157161)  (5,0.99366953)};
      \end{axis}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,axis y line*=right,axis x line=none,ylabel=Variance,yticklabel style=blue,ylabel style=blue, y axis line style=blue,ytick style=blue]
        \addplot[thick,mark=*,blue] coordinates { (1,0.58318064)  (2,0.36100695)  (3,0.04437561)  (4,0.00300841)  (5,0.00209792)};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- Principal components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.9\textwidth,height=0.3\textwidth,grid,xmin=400,xmax=900,cycle list name=color list]
        \addplot+[thick] table[col sep=comma,x index=0,y index=1] {figures/pca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=2] {figures/pca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=3] {figures/pca_pcs.csv};
        \legend{pc1,pc2,pc3};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** PCA case study 3/3
#+BEGIN_SRC python :tangle ../Codes/pcaPavia.py
# Projection of the first PCs
imp = sp.dot(im,pca.components_[:3,:].T)
imp.shape = (h,w,3)

# Save image
rt.write_data('../Data/pca_university.tif',imp,GeoT,Proj)
#+END_SRC
**** PCA 1                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_pc1.png]]

**** PCA 2                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_pc2.png]]

**** PCA 3                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_pc3.png]]

*** Kernel PCA
- PCA is limited to second order information
- To capture higher-order statistics, it is possible to map the data onto another space $\mathcal{H}$
  #+BEGIN_EXPORT latex
    \begin{eqnarray*}
      \begin{array}{rcl}
        \phi:\mathbb{R}^d &\to&\mathcal{H}\\
        \mathbf{x}&\mapsto&\phi(\mathbf{x}).
      \end{array}
    \end{eqnarray*}
  #+END_EXPORT
- In $\mathcal{H}$, conventional PCA can be applied.
- Using the /kernel trick/ it is possible to directly work on the /kernel matrix/ in $\mathbb{R}^d$
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}\label{kpca:matrix}
   \mathbf{K}=\left(
   \begin{array}{cccc}
   k(\mathbf{x}_1,\mathbf{x}_1) & k(\mathbf{x}_1,\mathbf{x}_2) & \ldots & k(\mathbf{x}_1,\mathbf{x}_n)\\
   k(\mathbf{x}_2,\mathbf{x}_1) & k(\mathbf{x}_2,\mathbf{x}_2) & \ldots & k(\mathbf{x}_2,\mathbf{x}_n)\\ 
   \vdots & \vdots & \ddots & \vdots \\
   k(\mathbf{x}_n,\mathbf{x}_1) & k(\mathbf{x}_n,\mathbf{x}_2) & \ldots & k(\mathbf{x}_n,\mathbf{x}_n)\\
   \end{array}\right).
  \end{eqnarray*}
  #+END_EXPORT
- <2> KPCA versus PCA:

  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tabular}{ccc}
    \begin{tikzpicture}
      \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,grid]
        \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/kpca_data.csv};
      \end{axis}
    \end{tikzpicture}&
    \begin{tikzpicture}
      \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,grid]
        \addplot[scatter,only marks,scatter src=explicit] table[col sep =comma,meta index=2,x index=0,y index=1] {figures/kpca_datap.csv};
      \end{axis}
    \end{tikzpicture}&
                       \begin{tikzpicture}
        \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,ylabel=\% of variance,axis y line*=left,yticklabel style=red,ylabel style=red, y axis line style=red,ytick style=red]
          \addplot[thick,mark=*,red] coordinates { (1,0.171950045779)
            (2,0.293633371022)
            (3,0.41194893578)
            (4,0.481444806977)
            (5,0.54956124474)
            (6,0.612183510855)
            (7,0.673659036749)
            (8,0.721296411495)
            (9,0.767653893262)
            (10,0.80191080235)};
        \end{axis}
        \begin{axis}[width=0.3\textwidth,height=0.3\textwidth,axis y line*=right,axis x line=none,ylabel=Variance,yticklabel style=blue,ylabel style=blue, y axis line style=blue,ytick style=blue]
          \addplot[thick,mark=*,blue] coordinates {(1,57.4446834929)
            (2,40.6516908637)
            (3,39.5265970358)
            (4,23.2170239146)
            (5,22.7561859043)
            (6,20.9207054308)
            (7,20.5376050443)
            (8,15.914586718)
            (9,15.4870029584)
            (10,11.4444709279) };
        \end{axis}
      \end{tikzpicture}         
    \end{tabular}

  \end{center}
  #+END_EXPORT

*** Kernel PCA in practice
- Choose the kernel and its parameters
- Compute the kernel matrix $\mathbf{K}$ for all the pixels (or a subset)
- Center the matrix
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
   \mathbf{K}_c=\mathbf{K}-\mathbf{1}_n\mathbf{K}-\mathbf{K}\mathbf{1}_n+\mathbf{1}_n\mathbf{K}\mathbf{1}_n
  \end{eqnarray*}
  #+END_EXPORT
- Solve the eigenproblems
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \lambda\boldsymbol{\alpha}=\mathbf{K}\boldsymbol{\alpha} \text{ subject to } \|\boldsymbol{\alpha}\|_2 = \frac{1}{\lambda}
  \end{eqnarray*}
  #+END_EXPORT
- Project on the $p$ first /kernel principal components/: $\phi^{kpc}(\mathbf{x})=\begin{bmatrix}\phi^{kpc}_1(\mathbf{x})&\hdots&\phi^{kpc}_p(\mathbf{x})\end{bmatrix}^t$
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \phi^{kpc}_j(\mathbf{x})=\sum_{i=1}^n \alpha_{ki} k(\mathbf{x}_i,\mathbf{x})
  \end{eqnarray*}
  #+END_EXPORT

*** KPCA case study 1/3

From cite:fauvel2009kernel.

#+BEGIN_SRC python :tangle ../Codes/kpcaPavia.py
import rasterTools as rt
import scipy as sp
from sklearn.decomposition import KernelPCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Load data set
im,GeoT,Proj = rt.open_data('../Data/university.tif')
[h,w,b]=im.shape
im.shape=(h*w,b)
wave = sp.loadtxt('../Data/waves.csv',delimiter=',')

# Scale data
sc = StandardScaler()
im = sc.fit_transform(im)

# Do KPCA
kpca = KernelPCA(kernel='rbf',gamma=1.0/b,n_jobs=-1)
kpca.fit(im[::50,:]) # Use a subset of the total number of pixels

#+END_SRC
#+BEGIN_SRC python :tangle ../Codes/kpcaPavia.py :exports none
# Plot explained variance
l = kpca.lambdas_
cl = l.cumsum()/l.sum()
for i in range(10):
    print "({0},{1})".format(i+1,l[i])

for i in range(10):
    print "({0},{1})".format(i+1,cl[i])

# Save Eigenvectors
idx = sp.arange(kpca.alphas_[0,:].size)+1
D = sp.concatenate((idx[:,sp.newaxis],kpca.alphas_[:3,:].T),axis=1)
sp.savetxt('../FeatureExtraction/figures/kpca_pcs.csv',D,delimiter=',')
#+END_SRC

*** KPCA case study 2/3
- Explained variance
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,ylabel=\% of variance,xlabel=Number of principal components,axis y line*=left,yticklabel style=red,ylabel style=red, y axis line style=red,ytick style=red]
        \addplot[thick,mark=*,red] coordinates {(1,0.257631571125)
        (2,0.438129567049)
        (3,0.61420975716)
        (4,0.695091757082)
        (5,0.751533118467)
        (6,0.790148033382)
        (7,0.814644462352)
        (8,0.833924207631)
        (9,0.851128186791)
        (10,0.865878267501) };
      \end{axis}
      \begin{axis}[width=0.5\textwidth,height=0.25\textwidth,axis y line*=right,axis x line=none,ylabel=Variance,yticklabel style=blue,ylabel style=blue, y axis line style=blue,ytick style=blue]
        \addplot[thick,mark=*,blue] coordinates {(1,649.766197024)
        (2,455.229519695)
        (3,444.087481204)
        (4,203.990486367)
        (5,142.349357966)
        (6,97.389719371)
        (7,61.7818360658)
        (8,48.6249674864)
        (9,43.3897292304)
        (10,37.2008127987) };
        \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- Principal components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.9\textwidth,height=0.3\textwidth,grid,cycle list name=color list,xmin=0,xmax=4148]
        \addplot+[thick] table[col sep=comma,x index=0,y index=1] {figures/kpca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=2] {figures/kpca_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=3] {figures/kpca_pcs.csv};
        \legend{kpc1,kpc2,kpc3};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** KPCA case study 3/3
#+BEGIN_SRC python :tangle ../Codes/kpcaPavia.py
imp = kpca.transform(im)[:,:3]
imp.shape = (h,w,3)

# Save image
rt.write_data('../Data/kpca_university.tif',imp,GeoT,Proj)
#+END_SRC
**** KPCA 1                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_kpc1.png]]

**** KPCA 2                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_kpc2.png]]

**** KPCA 3                                                        :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_kpc3.png]]

** Supervised
*** Fisher's Discriminant Analysis
- We observe some $\{\mathbf{x}_i,y_i\}_{i=1}^n$
- Use the label information to find the linear features that highlight differences among classes

  #+BEGIN_EXPORT latex 
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.6\textwidth,grid,small,xmin=-5,xmax=5,ymin=-5,ymax=5]
        \addplot[scatter, only marks,scatter src=explicit]table[col sep=comma,meta index=2,x index =0,y index=1] {figures/lda_data.csv};      
        \addplot[domain=-5:5,very thick] {-x/0.52306077251960925*0.85229538790913895 - 3.65687201/3.04406312};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
- FDA: find $\mathbf{a}$ such as the ratio between the /between projected variance/ and the /sample projected variance/ is maximal
*** FDA Algorithm
- Between-class covariance matrix:
  #+BEGIN_EXPORT latex
  \begin{eqnarray*}
    \mathbf{B} = \frac{1}{n}\sum_{c=1}^Cn_c(\boldsymbol{\mu}_c-\boldsymbol{\mu})(\boldsymbol{\mu}_c-\boldsymbol{\mu})^\top
  \end{eqnarray*}
  #+END_EXPORT
- Sample covariance matrix
  #+BEGIN_EXPORT latex
   \begin{eqnarray*}
     \boldsymbol{\Sigma} = \frac{1}{n-1}\sum_{i=1}^n(\mathbf{x}_i-\boldsymbol{\mu})(\mathbf{x}_i-\boldsymbol{\mu})^\top
   \end{eqnarray*}
   #+END_EXPORT
- The Fisher discriminant subspace is given by the eigenvectors of $\boldsymbol{\Sigma}^{(-1)}\mathbf{B}$
- Remark: there are at most $C-1$ eigenvectors because $\text{Rank}(\mathbf{B})\leq C-1$.
*** FDA case study 1/3

#+BEGIN_SRC python :tangle ../Codes/ldaPavia.py
import rasterTools as rt
import scipy as sp
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Load data set
X,y=rt.get_samples_from_roi('../Data/university.tif','../Data/university_gt.tif')
wave = sp.loadtxt('../Data/waves.csv',delimiter=',')

# Select the same number of samples
nt = 900
xt,yt=[],[]
for i in sp.unique(y):
    t = sp.where(y==i)[0]
    nc = t.size
    rp =  sp.random.permutation(nc)
    xt.extend(X[t[rp[0:nt]],:])
    yt.extend(y[t[rp[0:nt]]])

xt = sp.asarray(xt)
yt = sp.asarray(yt)

# Do LDA
lda = LinearDiscriminantAnalysis(solver='eigen',shrinkage='auto')
lda.fit(xt,yt.ravel())
#+END_SRC
#+BEGIN_SRC python :tangle ../Codes/ldaPavia.py :exports none
# Plot explained variance
l = lda.explained_variance_ratio_
cl= l.cumsum()

for i in range(y.max()-1):
    print "({0},{1})".format(i+1,l[i])

for i in range(y.max()-1):
    print "({0},{1})".format(i+1,cl[i])

# Projet data
import matplotlib.pyplot as plt
xp=lda.transform(xt)

# Save projection
D = sp.concatenate((xp[::10,:4],yt[::10]),axis=1)
sp.savetxt("../FeatureExtraction/figures/lda_proj.csv",D,delimiter=',')

# Save Eigenvectors
D = sp.concatenate((wave[:,sp.newaxis],lda.coef_[:3,:].T),axis=1)
sp.savetxt('../FeatureExtraction/figures/lda_pcs.csv',D,delimiter=',')
#+END_SRC

#+RESULTS:
*** FDA case study 2/3
- Projection on Fisher components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tabular}{cc}
   \begin{tikzpicture}
        \begin{axis}[width=0.4\textwidth,height=0.3\textwidth,xticklabels={,,},yticklabels={,,},grid,xlabel=FC 1,ylabel=FC 2]
          \addplot[scatter,only marks,scatter src=explicit,opacity=0.5] table[col sep =comma,meta index=4,x index=0,y index=1] {figures/lda_proj.csv};
        \end{axis}
      \end{tikzpicture}&
                         \begin{tikzpicture}
                           \begin{axis}[width=0.4\textwidth,height=0.3\textwidth,xticklabels={,,},yticklabels={,,},grid,,xlabel=FC 3,ylabel=FC 4]
                             \addplot[scatter,only marks,scatter src=explicit,opacity=0.5] table[col sep =comma,meta index=4,x index=3,y index=2] {figures/lda_proj.csv};
                           \end{axis}
                         \end{tikzpicture}
    \end{tabular}
  \end{center}
  #+END_EXPORT
- Fisher components
  #+BEGIN_EXPORT latex
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[width=0.9\textwidth,height=0.3\textwidth,grid,xmin=400,xmax=900,cycle list name=color list]
        \addplot+[thick] table[col sep=comma,x index=0,y index=1] {figures/lda_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=2] {figures/lda_pcs.csv};
        \addplot+[thick] table[col sep=comma,x index=0,y index=3] {figures/lda_pcs.csv};
        \legend{pc1,pc2,pc3};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  #+END_EXPORT
*** FDA case study 3/3
#+BEGIN_SRC python :tangle ../Codes/ldaPavia.py
im,GeoT,Proj = rt.open_data('../Data/university.tif')
[h,w,b]=im.shape
im.shape=(h*w,b)
imp = lda.transform(im)[:,:3]
imp.shape = (h,w,3)
# Save image
rt.write_data('../Data/lda_university.tif',imp,GeoT,Proj)
#+END_SRC
**** LDA 1                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_lda1.png]]

**** LDA 2                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_lda2.png]]

**** LDA 3                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.3
:END:
#+ATTR_LATEX: :width 0.75\linewidth :center
[[file:./figures/university_lda3.png]]


* Spatial feature extaction

** Linear filters

** Mathematical morphology
* Figures                                                          :noexport:
** PCA
#+BEGIN_SRC python :session :results output
# Load data
from sklearn.datasets.samples_generator import make_classification
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import scipy as sp


# Generate samples
X, y = make_classification(n_samples=100, n_classes=1, n_features=2, n_informative=2,n_redundant=0,random_state=1,n_clusters_per_class=1)

# Do PCA
pca = PCA(n_components=2)
pca.fit(X)

print(pca.explained_variance_ratio_)
print(pca.components_)

# Compute the eigenvector
for i in range(2):
      print(X.mean(axis=0)-pca.components_[i])
      print(X.mean(axis=0)+pca.components_[i])
      

# Save data
sp.savetxt("figures/pca_data.csv",X,delimiter=',')
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> >>> >>> >>> >>> ... >>> >>> ... >>> PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
>>> [ 0.84182344  0.15817656]
[[ 0.9899841   0.14117892]
 [ 0.14117892 -0.9899841 ]]
>>> ... ... ... ... ... [ 0.080264    0.83834891]
[ 2.06023219  1.12070676]
[ 0.92906917  1.96951193]
[ 1.21142702 -0.01045626]
#+end_example
** KPCA
#+BEGIN_SRC python :session :results output
from sklearn.datasets import make_circles
from sklearn.decomposition import KernelPCA
import scipy as sp

# Data generation
X, y = make_circles(n_samples=400, factor=.3, noise=.05)
# Do KPCA
kpca = KernelPCA(kernel="rbf",gamma=5)
Xp = kpca.fit_transform(X)

# Get eigenvalues
l = kpca.lambdas_
cl = l.cumsum()/l.sum()

for i in range(10):
    print "({0},{1})".format(i+1,l[i])

for i in range(10):
    print "({0},{1})".format(i+1,cl[i])

# Save results
sp.savetxt("figures/kpca_data.csv",sp.concatenate((X,y[:,sp.newaxis]),axis=1),delimiter=',')
sp.savetxt("figures/kpca_datap.csv",sp.concatenate((Xp[:,:2],y[:,sp.newaxis]),axis=1),delimiter=',')
#+END_SRC

#+RESULTS:
#+begin_example

>>> >>> >>> ... >>> ... >>> >>> >>> ... >>> >>> >>> ... ... (1,57.4446834929)
(2,40.6516908637)
(3,39.5265970358)
(4,23.2170239146)
(5,22.7561859043)
(6,20.9207054308)
(7,20.5376050443)
(8,15.914586718)
(9,15.4870029584)
(10,11.4444709279)
... ... (1,0.171950045779)
(2,0.293633371022)
(3,0.41194893578)
(4,0.481444806977)
(5,0.54956124474)
(6,0.612183510855)
(7,0.673659036749)
(8,0.721296411495)
(9,0.767653893262)
(10,0.80191080235)
#+end_example
** FDA
#+BEGIN_SRC python :results output
from sklearn.datasets.samples_generator import make_classification
import scipy as  sp
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Generate data
x,y = make_classification(n_samples=200,n_classes=2,n_features=2,n_informative=2,n_redundant=0,n_clusters_per_class=1,random_state=1,class_sep=2)
x-=x.mean(axis=0)
# Save data
D = sp.concatenate((x,y[:,sp.newaxis]),axis=1)
sp.savetxt("figures/lda_data.csv",D,delimiter=',')
# Apply LDA
clf = LinearDiscriminantAnalysis(solver='eigen')
clf.fit(x, y)
print clf.coef_
print clf.intercept_
#+END_SRC

#+RESULTS:
: [[ 1.86816687 -3.04406312]]
: [-0.08378251]

